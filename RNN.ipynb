{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm, tqdm_notebook,notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/imdb_reviews.csv'\n",
    "if not Path(DATA_PATH).is_file():\n",
    "    gdd.download_file_from_google_drive(\n",
    "        file_id = '1zfM5E6HvKIe7f3rEt1V2gBpw5QOSSKQz',\n",
    "        dest_path = DATA_PATH \n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequences(Dataset):\n",
    "    def __init__(self, path, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        df = pd.read_csv(path)\n",
    "        vectorizer = CountVectorizer(stop_words='english', min_df = 0.015)\n",
    "        #self.dfs=df.review.tolist()\n",
    "        vectorizer.fit(df.review.tolist())\n",
    "        self.token2idx = vectorizer.vocabulary_\n",
    "        self.token2idx['<PAD>'] = max(self.token2idx.values()) +1\n",
    "        tokenizer = vectorizer.build_analyzer()\n",
    "        self.encode = lambda x: [self.token2idx[token] for token in tokenizer(x) if token in self.token2idx]\n",
    "        self.pad = lambda x : x +(max_seq_len - len(x)) * [self.token2idx['<PAD>']]\n",
    "        sequences = [self.encode(sequence)[:max_seq_len] for sequence in df.review.tolist()]\n",
    "        sequences , self.labels = zip(*[(sequence,label) for sequence, label in zip(sequences,df.label.tolist()) if sequence])\n",
    "        self.sequences =[self.pad(sequence) for  sequence in sequences]\n",
    "\n",
    "    def __getitem__(self, i ):\n",
    "            assert len(self.sequences[i]) == self.max_seq_len\n",
    "            return self.sequences[i] , self.labels[i]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Sequences(DATA_PATH,max_seq_len=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1104"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch ):\n",
    "\n",
    "    inputs = torch.LongTensor([item[0] for item in batch])\n",
    "    target = torch.FloatTensor([item[1] for item in batch])\n",
    "    return inputs , target\n",
    "\n",
    "batch_size = 2048\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size , \n",
    "        batch_size,\n",
    "        embedding_dimension =100,\n",
    "        hidden_size = 128,\n",
    "        n_layers = 1,\n",
    "        device='cpu'\n",
    "    ):\n",
    "\n",
    "        super(RNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dimension)\n",
    "        self.rnn = nn.GRU(\n",
    "            embedding_dimension, \n",
    "            hidden_size,\n",
    "            num_layers = n_layers, \n",
    "            batch_first= True,\n",
    "        )\n",
    "        self.decoder = nn.Linear(hidden_size ,1)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.randn(self.n_layers, self.batch_size, self.hidden_size).to(self.device)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size != self.batch_size:\n",
    "            self.batch_size = batch_size\n",
    "        encoded  = self.encoder(inputs)\n",
    "        print(\"enc\")\n",
    "        print(encoded.shape)\n",
    "        output, hidden = self.rnn(encoded, self.init_hidden())\n",
    "        print(\"output  \")\n",
    "        print(output.shape)\n",
    "        output = self.decoder(output[ : , : ,-1]).squeeze()\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (encoder): Embedding(1104, 100)\n",
       "  (rnn): GRU(100, 128, batch_first=True)\n",
       "  (decoder): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN(\n",
    "    hidden_size=128,\n",
    "    vocab_size= len(dataset.token2idx),\n",
    "    device=device,\n",
    "    batch_size = batch_size,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr = 0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c76d5303904672bfd38c6c8723abde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n",
      "enc\n",
      "torch.Size([2048, 128, 100])\n",
      "output  \n",
      "torch.Size([2048, 128, 128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [57], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m output \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     15\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[1;32m---> 17\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     19\u001b[0m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m3\u001b[39m)\n\u001b[0;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "train_losses = []\n",
    "for epoch in range(10):\n",
    "    progress_bar = notebook.tqdm(train_loader, leave=False)\n",
    "    losses = []\n",
    "    total = 0\n",
    "    #print(\"soop\")\n",
    "    for inputs, target in progress_bar:\n",
    "        inputs, target = inputs.to(device), target.to(device\n",
    "                                                     )\n",
    "        model.zero_grad()\n",
    "        #print(\"loop\")\n",
    "        output = model(inputs)\n",
    "    \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "              \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_description(f'Loss: {loss.item():.3f}')\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        total += 1\n",
    "    \n",
    "    epoch_loss = sum(losses) / total\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    tqdm.write(f'Epoch #{epoch + 1}\\tTrain Loss: {epoch_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_vector=torch.LongTensor([dataset.pad(dataset.encode(text))]).to(device)\n",
    "        output =model(test_vector)\n",
    "        prediction = torch.sigmoid(output).item()\n",
    "        if prediction > 0.5:\n",
    "            print(\"positive\")\n",
    "        else:\n",
    "            print(\"negative\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "test_text = \"\"\"\n",
    "This poor excuse for a movie is terrible. It has been 'so good it's bad' for a\n",
    "while, and the high ratings are a good form of sarcasm, I have to admit. But\n",
    "now it has to stop. Technically inept, spoon-feeding mundane messages with the\n",
    "artistic weight of an eighties' commercial, hypocritical to say the least, it\n",
    "deserves to fall into oblivion. Mr. Derek, I hope you realize you are like that\n",
    "weird friend that everybody know is lame, but out of kindness and Christian\n",
    "duty is treated like he's cool or something. That works if you are a good\n",
    "decent human being, not if you are a horrible arrogant bully like you are. Yes,\n",
    "Mr. 'Daddy' Derek will end on the history books of the internet for being a\n",
    "delusional sour old man who thinks to be a good example for kids, but actually\n",
    "has a poster of Kim Jong-Un in his closet. Destroy this movie if you all have a\n",
    "conscience, as I hope IHE and all other youtube channel force-closed by Derek\n",
    "out of SPITE would destroy him in the courts.This poor excuse for a movie is\n",
    "terrible. It has been 'so good it's bad' for a while, and the high ratings are\n",
    "a good form of sarcasm, I have to admit. But now it has to stop. Technically\n",
    "inept, spoon-feeding mundane messages with the artistic weight of an eighties'\n",
    "commercial, hypocritical to say the least, it deserves to fall into oblivion.\n",
    "Mr. Derek, I hope you realize you are like that weird friend that everybody\n",
    "know is lame, but out of kindness and Christian duty is treated like he's cool\n",
    "or something. That works if you are a good decent human being, not if you are a\n",
    "horrible arrogant bully like you are. Yes, Mr. 'Daddy' Derek will end on the\n",
    "history books of the internet for being a delusional sour old man who thinks to\n",
    "be a good example for kids, but actually has a poster of Kim Jong-Un in his\n",
    "closet. Destroy this movie if you all have a conscience, as I hope IHE and all\n",
    "other youtube channel force-closed by Derek out of SPITE would destroy him in\n",
    "the courts.\n",
    "\"\"\"\n",
    "predict_sentiment(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c1f741a4f83aa020b4b2a4d7353a073a4e5e4a855a3258a20da40294ddbf005"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
